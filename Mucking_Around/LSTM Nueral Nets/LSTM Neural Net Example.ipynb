{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import operator\n",
    "import numpy as np\n",
    "import nltk\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.layers import Input, Embedding, LSTM, Dense, merge\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dropout, Flatten, Convolution2D, MaxPooling2D, Dense, Activation\n",
    "from keras.layers import Convolution1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import plot\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary_size = 8000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentances = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 36675 sentences.\n"
     ]
    }
   ],
   "source": [
    "with open('reddit_comments.csv', 'r',  encoding=\"utf8\") as f:\n",
    "    reader = csv.reader(f, skipinitialspace=True)\n",
    "    reader.__next__()\n",
    "    # Split full comments into sentences\n",
    "    sentences = itertools.chain(*[nltk.sent_tokenize(x[0].lower()) for x in reader])\n",
    "    # Append SENTENCE_START and SENTENCE_END\n",
    "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
    "print(\"Parsed %d sentences.\" % (len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START i would actually love that. SENTENCE_END',\n",
       " 'SENTENCE_START c9 are my na team and najin were the first team i bothered to watch in esports so i want them to do it. SENTENCE_END',\n",
       " 'SENTENCE_START army surplus SENTENCE_END',\n",
       " 'SENTENCE_START huh. SENTENCE_END',\n",
       " 'SENTENCE_START never knew max was a vampire. SENTENCE_END',\n",
       " \"SENTENCE_START couldn't feel more ashamed. SENTENCE_END\",\n",
       " 'SENTENCE_START then we wonder where these racist, ignorant cunts like the ones showing up in this thread are coming from. SENTENCE_END',\n",
       " 'SENTENCE_START welcome! SENTENCE_END',\n",
       " 'SENTENCE_START what is your favorite could not live without it product? SENTENCE_END',\n",
       " 'SENTENCE_START i know it might grind your gears, but it currently is true. SENTENCE_END']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 15000/15000 [00:01<00:00, 13444.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# for sentance in tqdm(list(pd.read_csv('reddit_comments.csv')['body'])):\n",
    "#     sentances.append(nltk.sent_tokenize(sentance.lower()))\n",
    "\n",
    "    \n",
    "# sentances = itertools.chain(*sentances)\n",
    "# sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Count the word frequencies\n",
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the most common words and build index_to_word and word_to_index vectors\n",
    "vocab = word_freq.most_common(vocabulary_size-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(tokenized_sentences):\n",
    "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the training data\n",
    "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SENTENCE_START i would actually love that. SENTENCE_END',\n",
       " 'SENTENCE_START c9 are my na team and najin were the first team i bothered to watch in esports so i want them to do it. SENTENCE_END',\n",
       " 'SENTENCE_START army surplus SENTENCE_END',\n",
       " 'SENTENCE_START huh. SENTENCE_END',\n",
       " 'SENTENCE_START never knew max was a vampire. SENTENCE_END',\n",
       " \"SENTENCE_START couldn't feel more ashamed. SENTENCE_END\",\n",
       " 'SENTENCE_START then we wonder where these racist, ignorant cunts like the ones showing up in this thread are coming from. SENTENCE_END',\n",
       " 'SENTENCE_START welcome! SENTENCE_END',\n",
       " 'SENTENCE_START what is your favorite could not live without it product? SENTENCE_END',\n",
       " 'SENTENCE_START i know it might grind your gears, but it currently is true. SENTENCE_END']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36675,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36675,)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ [0, 4410, 31, 35, 313, 256, 8, 7999, 114, 3, 139, 256, 5, 2640, 6, 247, 14, 7999, 42, 5, 109, 72, 6, 26, 9, 2],\n",
       "       [0, 1965, 7768], [0, 1668, 2], [0, 142, 688, 871, 30, 7, 7999, 2],\n",
       "       [0, 104, 17, 172, 63, 4582, 2],\n",
       "       [0, 96, 57, 821, 150, 165, 1102, 4, 2386, 7999, 44, 3, 379, 1270, 62, 14, 23, 483, 31, 429, 61, 2],\n",
       "       [0, 1163, 33], [0, 50, 13, 41, 515, 104, 29, 345, 232, 9, 993, 18],\n",
       "       [0, 5, 82, 9, 200, 5283, 41, 5742, 4, 24, 9, 644, 13, 328, 2]], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "SENTENCE_START : (\n",
      "[0, 20, 22]\n",
      "\n",
      "y:\n",
      ": ( SENTENCE_END\n",
      "[20, 22, 1]\n"
     ]
    }
   ],
   "source": [
    "x_example, y_example = X_train[17], y_train[17]\n",
    "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in x_example]), x_example))\n",
    "print (\"\\ny:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in y_example]), y_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 4000\n",
    "maxlen = 250\n",
    "batch_size = 32\n",
    "embedding_dims = 60\n",
    "nb_filter = 250\n",
    "filter_length = 3\n",
    "hidden_dims = 250\n",
    "nb_epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-4)\n",
    "objective = 'binary_crossentropy'\n",
    "\n",
    "\n",
    "text_seq_input =  Input(shape=(36675,), name = 'text_seq_input')\n",
    "\n",
    "model_1 = Embedding(max_features,\n",
    "                    embedding_dims,\n",
    "                    input_length=maxlen,\n",
    "                    dropout=0.1)(text_seq_input)\n",
    "\n",
    "#model.add(LSTM(32, input_shape=(10, 64)))\n",
    "\n",
    "#model.add(LSTM(16))\n",
    "\n",
    "model_1 = LSTM(100, return_sequences=True)(model_1)\n",
    "\n",
    "# model_1 = MaxPooling1D(pool_length=2, border_mode='valid')(model_1)\n",
    "\n",
    "# model_1 = Convolution1D(nb_filter=120,\n",
    "#                         filter_length=4,\n",
    "#                         border_mode='valid',\n",
    "#                         activation='relu',\n",
    "#                         subsample_length=1)(model_1)\n",
    "\n",
    "#model = MaxPooling1D(pool_length=2, border_mode='valid')(model)\n",
    "\n",
    "# model = Convolution1D(nb_filter=60,\n",
    "#                         filter_length=5,\n",
    "#                         border_mode='valid',\n",
    "#                         activation='relu',\n",
    "#                         subsample_length=1)(model)\n",
    "\n",
    "\n",
    "#model_1 = GlobalMaxPooling1D()(model_1)\n",
    "#model_1 = MaxPooling1D(pool_length=2, border_mode='valid')(model_1)\n",
    "\n",
    "\n",
    "# model_1 = Dense(250, activation='relu')(model_1)\n",
    "# model_1 = Dropout(0.25)(model_1)\n",
    "\n",
    "# model_1 = Dense(50, activation='relu')(model_1)\n",
    "# model_1 = Dropout(0.4)(model_1)\n",
    "\n",
    "#model_1_out = Flatten()(model_1)\n",
    "#model_1_out = Flatten()(model_1)\n",
    "model = Activation('sigmoid')(model)\n",
    "\n",
    "\n",
    "model = Dense(3)(model)\n",
    "model = Activation('softmax')(model)\n",
    "\n",
    "\n",
    "class_model = Model(input = [text_seq_input, clustered_input, text_matrix_input], output = [model, aux_1_output, aux_2_output, aux_3_output])\n",
    "class_model.compile(loss=objective, optimizer=optimizer, metrics=['accuracy'], loss_weights = [0.5, 0.25, 0.25, 0.25])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
