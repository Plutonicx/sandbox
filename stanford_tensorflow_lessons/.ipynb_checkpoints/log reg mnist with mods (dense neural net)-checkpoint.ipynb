{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist\\train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist\\train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist\\t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "MNIST = input_data.read_data_sets(\"../data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: Define parameters for the model\n",
    "learning_rate = 0.2\n",
    "batch_size = 128\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 3: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9.\n",
    "# each label is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784])\n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 4: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w_1 = tf.Variable(tf.random_normal(shape=[784, 100], stddev=1), name=\"weights_1\")\n",
    "b_1 = tf.Variable(tf.zeros([1, 100]), name=\"bias_1\")\n",
    "\n",
    "w_2 = tf.Variable(tf.random_normal(shape=[100, 30], stddev=1), name=\"weights_2\")\n",
    "b_2 = tf.Variable(tf.zeros([1, 30]), name=\"bias_2\")\n",
    "\n",
    "w_3 = tf.Variable(tf.random_normal(shape=[30, 10], stddev=1), name=\"weights_3\")\n",
    "b_3 = tf.Variable(tf.zeros([1, 10]), name=\"bias_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 5: predict Y from X and w, b\n",
    "# the model that returns probability distribution of possible label of the image\n",
    "# through the softmax layer\n",
    "# a batch_size x 10 tensor that represents the possibility of the digits\n",
    "sigmoid_first = tf.sigmoid(tf.matmul(X, w_1) + b_1)\n",
    "sigmoid_second = tf.sigmoid(tf.matmul(sigmoid_first, w_2) + b_2)\n",
    "logits = tf.matmul(sigmoid_second, w_3) + b_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 6: define loss function\n",
    "# use softmax cross entropy with logits as the loss function\n",
    "# compute mean cross entropy, softmax is applied internally\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over examples in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 7: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize cost\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.6769\n",
      "Accuracy 0.7278\n",
      "Accuracy 0.7632\n",
      "Accuracy 0.7864\n",
      "Accuracy 0.8088\n",
      "Accuracy 0.8249\n",
      "Accuracy 0.8372\n",
      "Accuracy 0.844\n",
      "Accuracy 0.8522\n",
      "Accuracy 0.8574\n",
      "Accuracy 0.8644\n",
      "Accuracy 0.8689\n",
      "Accuracy 0.8755\n",
      "Accuracy 0.8782\n",
      "Accuracy 0.8825\n",
      "Accuracy 0.8862\n",
      "Accuracy 0.8886\n",
      "Accuracy 0.8937\n",
      "Accuracy 0.8958\n",
      "Accuracy 0.8979\n",
      "Accuracy 0.9027\n",
      "Accuracy 0.9048\n",
      "Accuracy 0.9067\n",
      "Accuracy 0.9074\n",
      "Accuracy 0.9103\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_batches = int(MNIST.train.num_examples/batch_size)\n",
    "    for i in range(n_epochs): # train the model n_epochs times\n",
    "        for _ in range(n_batches):\n",
    "            X_batch, Y_batch = MNIST.train.next_batch(batch_size)\n",
    "            sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch})\n",
    "            \n",
    "# average loss should be around 0.35 after 25 epochs\n",
    "# test the model\n",
    "    n_batches = int(MNIST.test.num_examples/batch_size)\n",
    "    total_correct_preds = 0\n",
    "    for i in range(n_batches):\n",
    "        X_batch, Y_batch = MNIST.test.next_batch(batch_size)\n",
    "        _, loss_batch, logits_batch = sess.run([optimizer, loss, logits],\n",
    "feed_dict={X: X_batch, Y:Y_batch})\n",
    "        preds = tf.nn.softmax(logits_batch)\n",
    "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "        accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # similar\n",
    "#to numpy.count_nonzero(boolarray) :(\n",
    "        total_correct_preds += sess.run(accuracy)\n",
    "    print(\"Accuracy {0}\".format(total_correct_preds/MNIST.test.num_examples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
